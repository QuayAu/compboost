---
title: "Early Stopping"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Early stopping with compboost}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
  # fig.path = "Readme_files/"
)

library(compboost)
```

## Before Starting

- Read the [use-case](https://danielschalk.com/compboost/articles/getting_started/use_case.html) site to get to know how to define a `Compboost` object using the `R6` interface

## Data: Titanic Passenger Survival Data Set

We use the [titanic dataset](https://www.kaggle.com/c/titanic/data) with binary
classification on `Survived`. First of all we store the train and test data
in two data frames and remove all rows that contains `NA`s:

```{r}
# Store train and test data:
df = na.omit(titanic::titanic_train)
df$Survived = factor(df$Survived, labels = c("no", "yes"))
```

For the later stopping we split the dataset into train and test:

```{r}
set.seed(123)
idx_train = sample(seq_len(nrow(df)), size = nrow(df) * 0.8)
idx_test = setdiff(seq_len(nrow(df)), idx_train)
```

## Defining the Model

We define the same model as in the [use-case](https://danielschalk.com/compboost/articles/getting_started/use_case.html) but now just on the train index without specifying an out of bag fraction:
```{r}
cboost = Compboost$new(data = df[idx_train, ], target = "Survived", loss = LossBinomial$new())

cboost$addBaselearner("Age", "spline", BaselearnerPSpline)
cboost$addBaselearner("Fare", "spline", BaselearnerPSpline)
cboost$addBaselearner("Sex", "categorical", BaselearnerPolynomial, intercept = FALSE)
```


## Early Stopping in Compboost

### How does it work?

The early stopping of the boosting algorithm is done using the logger. The logger is executed after each iteration and stores class dependent data, e.g. the runtime. Additionally, each logger can declared as a stopper with `use_as_stopper = TRUE`. Declaring a logger as stopper, the logged data is used to stop the algorithm after a logger-specific criteria is reached. For example, using `LoggerTime` as stopper the algorithm can be stopped after a pre-defined runtime is reached.

### Example with runtime stopping

Now its time to define a logger to log the runtime. As mentioned above, we set `use_as_stopper = TRUE`. Now it matters what we specify in `max_time` since this is our stopping criteria. Here we like to stop after 50000 microseconds:

```{r, warnings=FALSE}
cboost$addLogger(logger = LoggerTime, use_as_stopper = TRUE, logger_id = "time",
  max_time = 50000, time_unit = "microseconds")

cboost$train(2000, trace = 100)
cboost
```

As we can see, the fittings is stopped after `r cboost$getCurrentIteration()` and not after 2000 as specified in train. Taking a look at the logger data we can also see that the last entry exceeds the 50000 microseconds and therefore hits the stopping criteria:
```{r}
tail(cboost$getLoggerData())
```


## Loss-Based Early Stopping

```{r, include=FALSE}
cboost = Compboost$new(data = df[idx_train, ], target = "Survived", loss = LossBinomial$new())

cboost$addBaselearner("Age", "spline", BaselearnerPSpline)
cboost$addBaselearner("Fare", "spline", BaselearnerPSpline)
cboost$addBaselearner("Sex", "categorical", BaselearnerPolynomial, intercept = FALSE)
```

In machine learning we often like to stop when the best model performance is reached. Especially in boosting, which tends to overfit, we need ether tuning or early stopping to determine what is a good number of iteration $m$ to gain good model performance. A well-known procedure is to log the out of bag (oob) behavior of the model and stop after this behavior starts to get worse. This is how early stopping is implemented in `compboost` in regard of the loss. The freedom we have is to specify

- the loss $L$ that is used for stopping: $$\mathcal{R}_{\text{emp}}^{[m]} = \frac{1}{n}\sum_{i=1}^n L\left(y^{(i)}, f^{[m]}(x^{(i)})\right)$$

- the percentage of performance increase that should be undershot: $$\text{err}^{[m]} = \frac{\mathcal{R}_{\text{emp}}^{[m- 1]} - \mathcal{R}_{\text{emp}}^{[m]}}{\mathcal{R}_{\text{emp}}^{[m - 1]}}$$

### Define the risk logger

Since we are interested in the oob behavior it is necessary to define the oob data and response in a manner that `compboost` understands it. Therefore, it is possible to use the `$prepareResponse()` and `$prepareData()` to get the objects:

```{r}
oob_response = cboost$prepareResponse(df$Survived[idx_test])
oob_data = cboost$prepareData(df[idx_test,])
```

With that data we can add the oob risk logger, declare it as stopper, and train the model:

```{r}
cboost$addLogger(logger = LoggerOobRisk, use_as_stopper = TRUE, logger_id = "oob",
  used_loss = LossBinomial$new(), eps_for_break = 0, patience = 5, oob_data = oob_data, 
  oob_response = oob_response)

cboost$train(2000, trace = 100)
```

**Note:** The use of `eps_for_break = 0` is a hard constrained that says that the training should continue until the oob risk starts to get bigger.

Taking a look at the logger data tells us that we stopped after the first 5 differences are bigger than zero (the oob risk of that iterations is bigger than the previous ones):
```{r}
tail(cboost$getLoggerData(), n = 10)
diff(tail(cboost$getLoggerData()$oob, n = 10))
```

```{r}
library(ggplot2)

ggplot(data = cboost$getLoggerData(), aes(x = `_iterations`, y = oob)) +
  geom_line() +
  xlab("Iteration") +
  ylab("Empirical Risk")
```

Taking a look at 2000 iterations shows that we have stopped quiet good:
```{r}
cboost$train(2000, trace = 0)

ggplot(data = cboost$getLoggerData(), aes(x = `_iterations`, y = oob)) +
  geom_line() +
  xlab("Iteration") +
  ylab("Empirical Risk")
```

**Note:** It could happen that the model's oob behavior increases locally for a view iterations and then starts to decrease again. To capture this we would need the "patience" parameter which waits for, lets say, 5 iterations and breaks just if all 5 iterations fulfill the criteria. Setting this parameter to 1 can lead to very unstable results:
```{r}
df = na.omit(titanic::titanic_train)
df$Survived = factor(df$Survived, labels = c("no", "yes"))

set.seed(123)
idx_train = sample(seq_len(nrow(df)), size = nrow(df) * 0.8)
idx_test = setdiff(seq_len(nrow(df)), idx_train)

cboost = Compboost$new(data = df[idx_train, ], target = "Survived", loss = LossBinomial$new())

cboost$addBaselearner("Age", "spline", BaselearnerPSpline)
cboost$addBaselearner("Fare", "spline", BaselearnerPSpline)
cboost$addBaselearner("Sex", "categorical", BaselearnerPolynomial, intercept = FALSE)

oob_response = cboost$prepareResponse(df$Survived[idx_test])
oob_data = cboost$prepareData(df[idx_test,])

cboost$addLogger(logger = LoggerOobRisk, use_as_stopper = TRUE, logger_id = "oob",
  used_loss = LossBinomial$new(), eps_for_break = 0, patience = 1, oob_data = oob_data, 
  oob_response = oob_response)

cboost$train(2000, trace = 0)


library(ggplot2)
ggplot(data = cboost$getLoggerData(), aes(x = `_iterations`, y = oob)) +
  geom_line() +
  xlab("Iteration") +
  ylab("Empirical Risk")
```

### Further comments on risk logging

- Since we can define as many as logger as we like it is possible to define multiple risk logger regarding different loss functions.
- It is also possible to log performance measures with the risk logging mechanism. This is covered as advanced topic.

## Some remarks

- Early stopping can be done globally or locally:
    - *locally*: The algorithm stops after **the first** stopping criteria of any logger is reached
    - *globally*: The algorithm stops after **all** stopping criteria are reached
- Some arguments are ignored when the logger is not set as stopper, e.g. `max_time` from the time logger
- The logger functionality is summarized [here](https://danielschalk.com/compboost/articles/functionality/logger.html)
