---
output: github_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(
	collapse = TRUE,
	echo = FALSE,
	fig.align = "center",
	fig.width = 9,
	fig.height = 6,
	out.width = "100%"
)
```

# Benchmarking compboost vs. mboost

```{r, results="asis"}
cat(readd(my.system))
```

This document was automatically created using `drake`. To recreate this document just source `drake_runtime_benchmark.R`. 

## Runtime Benchmark

To access the raw results of the runtime benchmark you need to load the registry:

```{r, eval=FALSE}
loadRegistry("benchmark/runtime/benchmark_files")
```

After preprocessing the raw data are stored into a `data.frame` where each row represents a job with instances like the elapsed time and the dimension of the simulated data:

```{r}
runtime.bm = readd(raw.runtime.benchmark.data)
runtime.bm[sample(seq_len(nrow(runtime.bm)), 10), ] %>%
  knitr::kable()
```

The preprocessing can be reproduced by taking a look at how `raw.runtime.benchmark.data` was created within the `drake_runtime_benchmark.R` script. This also applies for the following graphics.

### Increasing Number of Iterations

```{r}
grid.draw(readd(runtime.plot.iterations))
```

### Increasing Number of Base-Learner

```{r}
grid.draw(readd(runtime.plot.ncols))
```

### Increasing Number of Observations

```{r}
grid.draw(readd(runtime.plot.nrows))
```

<!--
### Memory Benchmark
-->
