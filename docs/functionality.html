<!DOCTYPE html>
<html lang="en">

<head>

  <link rel="apple-touch-icon" sizes="57x57" href="favicon/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="favicon/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="favicon/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="favicon/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="favicon/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="favicon/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="favicon/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="favicon/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="favicon/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="favicon/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
  <link rel="manifest" href="favicon/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="favicon/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="style.css">
  
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href='//fonts.googleapis.com/css?family=Rokkitt:400,700|Lato:400,300' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Nixie+One" rel="stylesheet">
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script>
<script type="text/javascript" async
src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- include highlight.js -->
<link rel="stylesheet" href="highlight_js/styles/zenburn.css">
<script src="highlight_js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

</head>

<body>

  <!-- Header -->
  <div class="navbar">
    <a href="index.html">About compboost</a>
    <a href="">Functionality</a>
    <a href="benchmark.html">Benchmark</a>
    <a href="vignettes.html">Vignettes</a>
    <a href="cpp_man/html/index.html">C++ Documentation</a>
    <a class="headerref", href="https://github.com/schalkdaniel/compboost">View Source</a>
  </div>
  
  <div class="content-noline">

    <img align="right" src="images/cboost_hexagon.png" width="100px">

    <p>
      <a class="badge", href="https://travis-ci.org/schalkdaniel/compboost">
        <img src="https://api.travis-ci.org/schalkdaniel/compboost.svg?branch=master", alt="Build Status">
      </a>     
      <a class="badge", href="https://ci.appveyor.com/project/schalkdaniel/compboost">
        <img src="https://ci.appveyor.com/api/projects/status/github/schalkdaniel/compboost?branch=master&svg=true", alt="AppVeyor Build Status">
      </a>
      <a class="badge", href='https://coveralls.io/github/schalkdaniel/compboost?branch=master'>
        <img src='https://coveralls.io/repos/github/schalkdaniel/compboost/badge.svg?branch=master', alt='Coverage Status' />
      </a>
    </p>


    <h1>Functionality</h1>

    <h2>Base-Learners</h2>

    <table>
      <tr>
        <th>Base-Learner</th>
        <th>Description</th>
        <th>Arguments</th>
      </tr>
      <tr>
        <td><code>BaselearnerPolynomial</code></td>
        <td>
          Fit polynomial base-learner. Note that this base-learner takes just the power of the selected feature. Hence, there is no linear part if one selects, for instance, <code>degree = 2</code>.
        </td>
        <td>
          <dt><b>degree</b></dt> <dd>Polynomial degree of feature (default: 1)</dd>
          <dt><b>intercept</b></dt> <dd>Number of inner knots (default: TRUE)</dd>
        </td>
      </tr>
      <tr>
        <td><code>BaselearnerPSpline</code></td>
        <td>Fit (penalized) spline regression base-learners.</td>
        <td>
          <dt><b>degree</b></dt> <dd>Polynomial degree of bases (default: 3)</dd>
          <dt><b>n.knots</b></dt> <dd>Number of inner knots (default: 20)</dd>
          <dt><b>penalty</b></dt> <dd>Penalty parameter for smoother curves (default: 2)</dd>
          <dt><b>differences</b></dt> <dd>Number of penalized differences of the knots (default: 2)</dd>
        </td>
      </tr>
      <tr>
        <td><code>BaselearnerCustom</code></td>
        <td>Define a custom base-learner by using <code>R</code> functions.</td>
        <td>
          <dt><b>instantiate.fun</b></dt> <dd>Feature transformation, e.g., create spline bases in the case of a spline learner</dd> 
          <dt><b>train.fun</b></dt> <dd>Function to train on the data gained by <code>instantiate.fun</code></dd>
          <dt><b>predict.fun</b></dt> <dd>Function to predict on the object returned by <code>train.fun</code></dd>
          <dt><b>param.fun</b></dt> <dd>Function to extract parameters as matrix from the object returned by <code>train.fun</code></dd>
        </td>
      </tr>
      <tr>
        <td><code>BaselearnerCustomCpp</code></td>
        <td>Define a custom base-learner by using <code>C++</code> functions.</td>
        <td>
          <dt><b>instantiate.ptr</b></dt> <dd>External pointer containing the reference to the <code>C++</code> feature transformation</dd>
          <dt><b>train.ptr</b></dt> <dd>Function to train on the data obtained by <code>instantiate.ptr</code> which always returns the parameter as matrix</dd>
          <dt><b>predict.ptr</b></dt> <dd>Function to predict by using the parameter obtained by <code>trian.ptr</code></dd>
        </td>
      </tr>
    </table>

    <h2>Loss</h2>

    <table>
      <tr>
        <th>Loss</th>
        <th>Description</th>
        <th>Arguments</th>
      </tr>
      <tr>
        <td><code>LossQuadratic</code></td>
        <td>
          Using quadratic differences between prediction and response. This loss corresponds to the gaussian distribution and yields ordinary residuals as pseudo residuals.
        </td>
        <td>
          <dt><b>offset</b></dt> <dd>Custom offset for initializing the model</dd>
        </td>
      </tr>
      <tr>
        <td><code>LossAbsolute</code></td>
        <td>
          Using absolute differences between prediction and response. This loss corresponds to the laplace distribution and yields the sign as pseudo residuals.
        </td>
        <td>
          <dt><b>offset</b></dt> <dd>Custom offset for initializing the model</dd>
        </td>
      </tr>
      <tr>
        <td><code>LossBinomial</code></td>
        <td>
          This loss can be used for binary classification and corresponds to the binomial distribution with logit link. Note that the labels are coded as -1 and 1 which yields slightly differences to other algorithms.
        </td>
        <td>
          <dt><b>offset</b></dt> <dd>Custom offset for initializing the model</dd>
        </td>
      </tr>
      <tr>
        <td><code>LossCustom</code></td>
        <td>Define a custom loss by using <code>C++</code> functions.</td>
        <td>
          <dt><b>loss</b></dt> <dd><code>R</code> function which calculates the loss between true values and a prediction</dd>
          <dt><b>gradient</b></dt> <dd><code>R</code> function to calculate the gradient of the loss function with ture values and prediction as input</dd>
          <dt><b>constant.initializer</b></dt> <dd><code>R</code> function to compute the constant loss optimal initialization (using a custom offset suppress this function and returns just the offset)</dd>
        </td>
      </tr>      
      <tr>
        <td><code>LossCustomCpp</code></td>
        <td>Define a custom loss by using <code>C++</code> functions.</td>
        <td>
          <dt><b>loss.ptr</b></dt> <dd><code>C++</code> pointer to a function which calculates the loss between true values and a prediction</dd>
          <dt><b>gradient.ptr</b></dt> <dd><code>C++</code> pointer to a function to calculate the gradient of the loss function with ture values and prediction as input</dd>
          <dt><b>constant.initializer.ptr</b></dt> <dd><code>C++</code> pointer to a function to compute the constant loss optimal initialization (using a custom offset suppress this function and returns just the offset)</dd>
        </td>
      </tr>
    </table>

    <h2>Logger</h2>

    <table>
      <tr>
        <th>Logger</th>
        <th>Description</th>
        <th>Arguments</th>
      </tr>
      <tr>
        <td><code>LoggerIteration</code></td>
        <td>
          The basic logger which loggs the actual iteration. This one is defined automatically by calling, e.g., <code>train(1000)</code>, as stopper with a maximal number of iteration as passed through <code>train()</code>.
        </td>
        <td>
          <dt><b>use.as.stopper</b></dt> <dd>Boolean to indicate whether this logger should also be used as stopper.</dd>
          <dt><b>max.iterations</b></dt> <dd>Integer value specifying the maximal numbers of iterations if the logger is used as stopper, otherwise this argument does not have any effect.</dd>
        </td>
      </tr>
      <tr>
        <td><code>LoggerInbagRisk</code></td>
        <td>
          This logger takes a <code>Loss</code> object and calculates the risk using the train dataset.
        </td>
        <td>
          <dt><b>use.as.stopper</b></dt> <dd>Boolean to indicate whether this logger should also be used as stopper.</dd>
          <dt><b>used.loss</b></dt> <dd><code>Loss</code> object which is used to calculate the risk. Note that this could also be a custom loss and therefore be used to track performance measures.</dd>
          <dt><b>eps.for.break</b></dt> <dd>Numeric value indicating the relative improvement which is used to stop the algorithm if the true improvement falls under <code>eps.for.break</code>.</dd>
        </td>
      </tr>      
      <tr>
        <td><code>LoggerOobRisk</code></td>
        <td>
          This logger takes a <code>Loss</code> object and calculates the risk using a custom out of bag dataset.
        </td>
        <td>
          <dt><b>use.as.stopper</b></dt> <dd>Boolean to indicate whether this logger should also be used as stopper.</dd>
          <dt><b>used.loss</b></dt> <dd><code>Loss</code> object which is used to calculate the risk. Note that this could also be a custom loss and therefore be used to track performance measures.</dd>
          <dt><b>eps.for.break</b></dt> <dd>Numeric value indicating the relative improvement which is used to stop the algorithm if the true improvement falls under <code>eps.for.break</code>.</dd>
          <dt><b>oob.data</b></dt> <dd>List of <code>Data</code> objects which corresponds to the same data objects used in <code>addBaselearner()</code>. Note that each <code>Compboost</code> object has a member function <code>prepareData()</code> which automatically converts a <code>data.frame</code> to a list required by the logger.</dd>
          <dt><b>oob.response</b></dt> <dd>Vector of the response values corresonding to <code>oob.data</code>.</dd>
        </td>
      </tr>
      <tr>
        <td><code>LoggerTime</code></td>
        <td>
          This logger can be used to measure the ellapsed time at each iteration.
        </td>
        <td>
          <dt><b>use.as.stopper</b></dt> <dd>Boolean to indicate whether this logger should also be used as stopper.</dd>
          <dt><b>max.time</b></dt> <dd>Integer value that indicates how much time the algorithm has for the training.</dd>
          <dt><b>time.unit</b></dt> <dd>Character to specify the time unit. Possible choices are minutes, seconds or microseconds.</dd>
        </td>
      </tr>
    </table>


<!--
  <h2>Optimizer</h2>

  <hr>
-->
</div> 
</body>
</html>
