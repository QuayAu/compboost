<!DOCTYPE html>
<html lang="en">

<head>

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet" type="text/css" href="style.css">

<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

</head>

<body>

<!-- Header -->
<div class="navbar">
  <a href="">About compboost</a>
  <a href="">Functionality</a>
  <a href="">Vignettes</a>
  <a href="cpp_man/html/index.html">C++ Documentation</a>
  <a class="headerref", href="https://github.com/schalkdaniel/compboost">View Source</a>
</div>

<div class="content-noline">


<p>
  <a class="badge", href="https://travis-ci.org/schalkdaniel/compboost">
    <img src="https://api.travis-ci.org/schalkdaniel/compboost.svg?branch=master", alt="Build Status">
  </a>
  <a class="badge", href='https://coveralls.io/github/schalkdaniel/compboost?branch=master'>
    <img src='https://coveralls.io/repos/github/schalkdaniel/compboost/badge.svg?branch=master', alt='Coverage Status' />
  </a>
</p>


<h1>compboost</h1>

<h2>What is model-based boosting?</h2>

<hr>

<p>
  Instead of getting huge predictive power, model-based (or component-wise) boosting aggregates statistical models to maintain interpretability. This is done by restricting the used base-learners to be linear in the parameters. This is important in terms parameter estimation. For instance, having a base-learner \(b_j(x, \theta^{[m]})\) of a feature \(x\) with a corresponding parameter vector \(\theta^{[m]}\) and another base-learner of the same type but with a different parameter vector \(b_j(x, \theta^{[m^\prime]})\), then it is possible, due to linearity, to combine those two learners to one new base-learner:
  \[
    b_j(x, \theta^{[m]}) + b_j(x, \theta^{[m^\prime]}) = b_j(x, \theta^{[m]} + \theta^{[m^\prime]})
  \] 
  So, instead of boosting trees as <code>xgboost</code> does to obtain huge predictive power, the result of model-based boosting is a selection of important base-learners each with estimated parameters. 
</p>
<p>
  The ordinary way of finding good base-learners is done by a greedy search which means that in each iteration all base-learners are fitted to the so called pseudo residuals that acts as some kind of errors. The selected one is then the base-learner with the smallest empirical risk \(\mathcal{R}_\text{emp}\). This greedy search and aggregation is illustrated below.
</p>
<img src="images/cboost.gif" class="center">
<p>
  The illustration above uses three base-learners \(b_1\), \(b_1\), and \(b_1\). You can think of each base-learner as wrapper around a feature or category which represents the effect of that feature on the target variable. Therefore, if a base-learner is selected more often than another one indicates that this base-learner (or feature) is more important than the other ones. To get a sparser model in terms of selected features there is also a learning rate \(\beta\) to shrink the parameter in each iteration. The learning rate corresponds to the step size used for gradient descent. The three iteration of the illustration are:
  \[
  \begin{align}
    \text{Iteration 1:} \ &\hat{f}^{[1]}(x) = \beta b_3(x_1, \theta^{[1]}) \\
    \text{Iteration 2:} \ &\hat{f}^{[2]}(x) = \beta b_3(x_1, \theta^{[1]}) + \beta b_3(x_1, \theta^{[2]}) \\
    \text{Iteration 2:} \ &\hat{f}^{[3]}(x) = \beta b_3(x_1, \theta^{[1]}) + \beta b_3(x_1, \theta^{[2]}) + \beta b_1(x_3, \theta^{[3]})
  \end{align}
  \]
  Using the linearity of the base-learners the final model results from aggregating the \(b_3\) base-learner:
  \[
    \hat{f}^{[3]}(x) = \beta \left( b_3(x_1, \theta^{[1]} + \theta^{[2]}) + b_1(x_3, \theta^{[3]}) \right)
  \]
</p>
<p>
  This is a very simple example but it displays the main strength of model-based boosting such as an inherent variable selection very well. Since the fitting is done iteratively on the single base-learners model-based boosting is also a very efficient model for data situations where \(p \gg n\), which is learning in high-dimensional feature spaces. For instance, genomic data where the number of features $p$ could be several thousand but the number of observations are not more than a few hundred.
</p>


<h2>Getting started with compboost</h2>

<hr>

</div>

</body>


</body>
</html>
